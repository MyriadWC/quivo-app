{"ast":null,"code":"import _slicedToArray from \"@babel/runtime/helpers/slicedToArray\";\nimport { useState, useEffect } from 'react';\nimport { frequencyIndexToComprehensionPercentage as f } from '../../../utils/functions';\nexport var useComprehensionPercentage = function useComprehensionPercentage(wordCounts, coeffs) {\n  var _useState = useState(null),\n    _useState2 = _slicedToArray(_useState, 2),\n    comprehensionPercentage = _useState2[0],\n    setComprehensionPercentage = _useState2[1];\n  var getComprehensionPercentage = function getComprehensionPercentage() {\n    var result = 0;\n    for (var i = 1; i <= 4001; i += 1000) {\n      result += f(i + wordCounts[`${i}-${i + 999}`], coeffs) - f(i, coeffs);\n    }\n    result += f(5001 + wordCounts['5000+'], coeffs) - f(5001, coeffs);\n    return result;\n  };\n  useEffect(function () {\n    setComprehensionPercentage(getComprehensionPercentage());\n  }, [wordCounts]);\n  return comprehensionPercentage;\n};","map":{"version":3,"names":["useState","useEffect","frequencyIndexToComprehensionPercentage","f","useComprehensionPercentage","wordCounts","coeffs","_useState","_useState2","_slicedToArray","comprehensionPercentage","setComprehensionPercentage","getComprehensionPercentage","result","i"],"sources":["C:/Users/Toby Usher/Documents/dev/quivo-app/frontend/screens/AccountScreen/hooks/useComprehensionPercentage.ts"],"sourcesContent":["import { useState, useEffect } from 'react';\r\n// Utils\r\nimport { frequencyIndexToComprehensionPercentage as f } from '../../../utils/functions';\r\n\r\nexport const useComprehensionPercentage = (wordCounts: Record<string, number>, coeffs: string[]) => {\r\n    const [comprehensionPercentage, setComprehensionPercentage] = useState<number | null>(null);\r\n\r\n    const getComprehensionPercentage = () => {\r\n        /*\r\n        All words in the corpus are listed in descending order of total appearance, and\r\n        normalised to between 0-100 to model the percentage of the corpus a user should\r\n        be able to understand with each additional new word learned, assuming the words\r\n        were learned from most frequent to least frequent, which is then modelled by \r\n        a + b/(1 + e**(c * n)), where n represents the frequency rank of a word in the\r\n        corpus, and a, b and c are constants that are determined for a given corpus to\r\n        model its word frequency distribution.\r\n        If the user learned the words of the corpus in perfect frequency order you could\r\n        work out their percentage comprehension by taking n to be the total number of\r\n        words they've learned so far, but since they'll learn new words in an unpredictable\r\n        order, their total comprehension can be found by summing each word's individual\r\n        contribution to overall corpus comprehension. For example, if the user knows the\r\n        nth most frequent word in the corpus, the additional percentage comprehension gained\r\n        by learning that word will be f(n) - f(n-1). By summing this value for all words\r\n        that the user knows, you can calculate the percentage of the corpus they should be\r\n        capable of understanding.\r\n\r\n        However, this is computationally expensive, so instead I've counted the number of\r\n        words a user knows in each section of 1000 words in descending order of frequency:\r\n        1-1000, 1001-2000, ... 5000+ most frequent etc. I then make the assumption that the\r\n        user learned the words in this 'bucket' in correct frequency order to arrive at an\r\n        estimate of the overall comprehension percentage that will still heavily discount\r\n        the least frequent words.\r\n        */\r\n        let result = 0;\r\n\r\n        for (let i = 1; i <= 4001; i += 1000) {\r\n            result += f(i + wordCounts[`${i}-${i+999}`], coeffs) - f(i, coeffs);\r\n        }\r\n        result += f(5001 + wordCounts['5000+'], coeffs) - f(5001, coeffs)\r\n\r\n        return result;\r\n    }\r\n\r\n    useEffect(() => {\r\n        setComprehensionPercentage(getComprehensionPercentage());\r\n    }, [wordCounts]);\r\n\r\n    return comprehensionPercentage;\r\n};"],"mappings":";AAAA,SAASA,QAAQ,EAAEC,SAAS,QAAQ,OAAO;AAE3C,SAASC,uCAAuC,IAAIC,CAAC,QAAQ,0BAA0B;AAEvF,OAAO,IAAMC,0BAA0B,GAAG,SAA7BA,0BAA0BA,CAAIC,UAAkC,EAAEC,MAAgB,EAAK;EAChG,IAAAC,SAAA,GAA8DP,QAAQ,CAAgB,IAAI,CAAC;IAAAQ,UAAA,GAAAC,cAAA,CAAAF,SAAA;IAApFG,uBAAuB,GAAAF,UAAA;IAAEG,0BAA0B,GAAAH,UAAA;EAE1D,IAAMI,0BAA0B,GAAG,SAA7BA,0BAA0BA,CAAA,EAAS;IA0BrC,IAAIC,MAAM,GAAG,CAAC;IAEd,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,IAAI,IAAI,EAAEA,CAAC,IAAI,IAAI,EAAE;MAClCD,MAAM,IAAIV,CAAC,CAACW,CAAC,GAAGT,UAAU,CAAE,GAAES,CAAE,IAAGA,CAAC,GAAC,GAAI,EAAC,CAAC,EAAER,MAAM,CAAC,GAAGH,CAAC,CAACW,CAAC,EAAER,MAAM,CAAC;IACvE;IACAO,MAAM,IAAIV,CAAC,CAAC,IAAI,GAAGE,UAAU,CAAC,OAAO,CAAC,EAAEC,MAAM,CAAC,GAAGH,CAAC,CAAC,IAAI,EAAEG,MAAM,CAAC;IAEjE,OAAOO,MAAM;EACjB,CAAC;EAEDZ,SAAS,CAAC,YAAM;IACZU,0BAA0B,CAACC,0BAA0B,CAAC,CAAC,CAAC;EAC5D,CAAC,EAAE,CAACP,UAAU,CAAC,CAAC;EAEhB,OAAOK,uBAAuB;AAClC,CAAC"},"metadata":{},"sourceType":"module","externalDependencies":[]}