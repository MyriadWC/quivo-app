{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the pretrained KMeans weights and use them to label the french sentences dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Toby\n",
      "[nltk_data]     Usher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load pretrained model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kmeans.pkl', 'rb') as f:\n",
    "    kmeans = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toby Usher\\AppData\\Local\\Temp\\ipykernel_30496\\2244699456.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filepath, delimiter='\\t', header=None)\n"
     ]
    }
   ],
   "source": [
    "filepath = os.path.join(\"..\", \"french_sentences.csv\")\n",
    "\n",
    "df = pd.read_csv(filepath, delimiter='\\t', header=None)\n",
    "df.columns = [\"id\", \"sentence\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Preprocess sentences\n",
    "\n",
    "Need to transform sentences into the same feature space as was used to train the kmeans model by applying the same stemming and vectorisation etc. that were used during training. Details of this can be found in the k means clustering training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'vous', 'par', 'aurions', 'la', 'des', 'nos', 'eussions', \"'\", 'serions', 'soit', 'se', 'étions', 'n', ':', 'fussent', 'pour', ']', 'son', 'aie', 'mon', 'fusses', 'nous', 'ayantes', 'm', 'soyez', 'fusse', 'ayants', 'eus', 'étais', ',', 'fussions', '\"', 'auraient', 'mes', 'avions', 'et', 'aux', 'aient', ')', 'ayant', 'aurai', 'sois', 'sera', 'seras', 'aviez', 'du', 'aurez', 'avons', 'auriez', 'aurais', 'étées', 'on', 'eûmes', 'ai', 'sont', 'je', 'fûmes', 'elle', 'sa', 'ne', 'ces', 'ayante', ';', 'avec', '!', 'êtes', 'd', 'eûtes', 'eue', '}', 'sur', 'l', 'j', 'me', 'que', 'étants', 'notre', '.', 'furent', 'avait', 'qu', 't', 'ton', 'étantes', 'étiez', 'aurons', 'ait', 'eut', 'ont', 'serait', 'fûtes', 'toi', 'il', 'soyons', 'un', 'fussiez', 'eux', 'à', 'serons', 'leur', 'au', 'étaient', 'moi', 'même', 'fût', 'étée', 'eues', 'ma', 'tes', 'auront', '?', 'serai', 'es', 'est', 'de', 'été', '{', 'avez', 'seraient', '%', 'ses', 'une', 'seront', 'fus', 'était', 'eurent', 'ayez', 'c', 'te', 'eussiez', 'sommes', 'serais', 'eusses', 'aurait', 'les', 'suis', 'vos', 'avaient', 'y', 'ayons', 'mais', 'dans', 'étant', 'seriez', 'ce', 'fut', 'aies', 'qui', 'étante', 'votre', 'serez', 'eussent', 'eu', 'ou', 'le', 'soient', 'ta', 'aura', 'avais', 'en', 'eût', 'as', 's', 'eusse', '[', 'ils', 'pas', 'lui', 'tu', 'étés', 'auras']\n"
     ]
    }
   ],
   "source": [
    "# First we need to get a list of stop words in french. These are common filler words that provide almost no useful information\n",
    "# and occur frequently in most documents. Stop words can have a disproportionate influence on the overall representation of\n",
    "# the document, which can be detrimental to the performance of TF-IDF. To mitigate this we need to remove stop words before\n",
    "# calculating TF-IDF vectors\n",
    "\n",
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "\n",
    "# Get French stop words\n",
    "french_stop_words = stopwords.words('french')\n",
    "\n",
    "# Combine with standard punctuation to get the comprehensive list of stop words. Convert stop words to a set\n",
    "# to ensure no duplicates\n",
    "stop_words = set(french_stop_words).union(punc)\n",
    "\n",
    "# convert back to list\n",
    "stop_words = list(stop_words)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw sentences from the dataframe\n",
    "sentences = df['sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('french')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def _tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Tokenizes a document into its individual words and punctuation and returns a list of each token's stem.\n",
    "    \"\"\"\n",
    "\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict clusters for each sentence and save dataframe as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Toby Usher\\Documents\\dev\\quivo-app\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aur', 'aurion', 'auron', 'avi', 'avion', 'avon', 'ayon', 'dan', 'e', 'euss', 'eussion', 'f', 'fuss', 'fussion', 'notr', 'ser', 'serion', 'seron', 'soi', 'somm', 'soyon', 'taient', 'tais', 'tait', 'tant', 'ti', 'tion', 'votr'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stop_words, tokenizer = _tokenize, max_features = 10000) # limit to 10000 most frequent terms in the corpus\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "df['cluster'] = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('french_sentences_with_cluster_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
