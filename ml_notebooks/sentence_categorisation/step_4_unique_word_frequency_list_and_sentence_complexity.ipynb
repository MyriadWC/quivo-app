{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to go through my cleaned word list, split all sentences into their individual words, and create a new dataframe containing each individual word and the number of occurrences within the dataset to determine dataset word frequency (Which I will take as a proxy for overall word frequency). This isn't perfect and will ignore context-defined meaning in words that are spelled the same, as this will be counted as only one highly frequent word with multiple meanings rather than several less-frequent words that are spelled the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few issues will need to be solved here.\n",
    "\n",
    "1. There will be typos and misspellings in the dataset which as well as being less useful as a learning tool, will also lead to some words (particularly those with accents that have been forgotten or misplaced) being counted multiple times, so they'll somehow need to be joined together. I can't just remove all accents as this can change the meaning of a word entirely (côté vs. côte). The approach I took was to just remove any sentences containing the least frequent words and then remove them from the corpus, which will remove a big chunk of typos, but the more common ones could still be in there.\n",
    "\n",
    "2. Words that have been shortened due to consecutive vowels (l, d etc. could have multiple meanings such as le or la, or de or du), but since these are incredibly common words that will always have high scores it might not be so much of an issue for gauging the word's frequency. I just need to make sure to check less frequent words that have been shortened i.e. the presque in presqu'île, although I imagine this shortening would be sufficiently infrequent relative to its unshortened form as to have almost no bearing on its frequency score.\n",
    "\n",
    "3. I might have to set some custom rules for when to keep an apostrophe. Returning to my previous example, presqu'île should really be treated as a single word, and other such exceptions will exist. I just need to go down the frequency list and note them down.\n",
    "\n",
    "4. Different word types will have differing numbers of forms. Adjectives can be masculine or feminine (public, publique), plus others such as publiquement. Verbs can have a very large number of forms, meaning etre is likely to be separated far more than most adjectives which will have fewer forms, which could lead to forms of etre appearing below far less common words in the frequency list. For a language learner these forms can sometimes be similar, allowing a transfer of knowledge when learning new forms (If they know courir it doesn't take much to figure out that couru is just the past tense), while other times the different forms might represent entirely new strings of characters that must all be learnt independently from one another (aller, irai etc.). I'd say that since these are effectively different words that each have to be learned, it is reasonable that they are treated separately. I'm effectively mapping the frequency of different strings of letters appearing in a language.\n",
    "\n",
    "While this method for measuring word frequency is flawed, as long as I ensure that the sentences in my database are grammatically correct it should be useful enough as a tool to gauge sentence complexity via average word frequency, which is the ultimate aim. I could also use POS tagging by running each sentence through a pretrained hidden markov model or something similar to label word type, and have the frequency for each word+type pair. This would avoid the issue of words with multiple meanings being classified together (est for is and east etc.). I have to do this later for the single-word translation to work so it would make sense to do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weird caching issue so overriding for now\n",
    "constants.language_code = 'fr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"../output_files/{constants.language_code}/step3_sentences.csv\"\n",
    "\n",
    "df = pd.read_csv(filepath, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     659855\n",
       "sentence               659855\n",
       "translated_sentence    659855\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regex\n",
    "\n",
    "fr_exceptions = [\n",
    "    \"[Aa]ujourd'hui\",\n",
    "    \"[Pp]resqu'île\",\n",
    "    \"[Qq]uelqu'un\",\n",
    "    \"[Dd]'accord\"\n",
    "    ]\n",
    "\n",
    "de_exceptions = []\n",
    "\n",
    "exceptions = {\n",
    "    'fr': fr_exceptions,\n",
    "    'de': de_exceptions\n",
    "    }[constants.language_code]\n",
    "\n",
    "word_regex = {\n",
    "    'fr': r'[a-zA-ZéèêëÉÈÊËàâäÀÂÄôöÔÖûüùÛÜÙçÇîÎïÏ]+',\n",
    "    'de': r'[a-zA-ZäöüÄÖÜß]+'\n",
    "    }[constants.language_code]\n",
    "\n",
    "exceptions_regex = '|'.join(exceptions)\n",
    "\n",
    "# Not ideal but de empty exceptions was causing issues\n",
    "regex = {\n",
    "    'fr': fr'\\b{exceptions_regex}|{word_regex}\\b',\n",
    "    'de': fr'\\b{word_regex}\\b',\n",
    "    }[constants.language_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_word_map = {\n",
    "    'j': 'je',\n",
    "    #'l': 'le', # Can be either so will handle after to speed up function\n",
    "    't': 'tu', # This will assign the t in a-t-on to tu for example, which will give tu a higher frequency than it should have, but it's only one very common word so I'm not going to address it\n",
    "    'd': 'de', # Need to check whether this is ever du\n",
    "    'c': 'ce',\n",
    "    's': 'se',\n",
    "    'qu': 'que',\n",
    "    'm': 'me',\n",
    "    'n': 'ne',\n",
    "    }\n",
    "\n",
    "def scan_sentence(sentence: str, unique_word_counts: defaultdict) -> defaultdict:\n",
    "    '''Scans a sentence to get its words and updates the unique word count dictionary.\n",
    "    Local unique_word_counts points to global variable so can update directly. The use\n",
    "    of a default dict means we don't have to check if a key is in the dictionary before\n",
    "    adding it as it will initialise to 1.\n",
    "\n",
    "    Using a dict instead of a dataframe means there's O(1) time complexity for insertions\n",
    "    and lookups, and using a defaultdict to avoid an additional check means this runs\n",
    "    incredibly quickly on even very large datasets.\n",
    "    '''\n",
    "\n",
    "    # Split all words in the sentence by word boundaries (Split uninclusively at punctuation or non-alphanumeric characters)\n",
    "    words = re.findall(regex, sentence) # Words only\n",
    "\n",
    "    # Set all words to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # Replace any shortened words with their full-length version\n",
    "    if constants.language_code == 'fr':\n",
    "        words = [shortened_word_map.get(word, word) for word in words]\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        # Add 1 to count. If word doesn't exist adds in new entry\n",
    "        unique_word_counts[word] += 1\n",
    "\n",
    "    return unique_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>translated_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16271971.0</td>\n",
       "      <td>Peut-être qu'elle paye toutes les vacheries qu...</td>\n",
       "      <td>Maybe it was her comeuppance for all the bad s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5690632.0</td>\n",
       "      <td>J'ai dit que j'ai commencé les arts martiaux à...</td>\n",
       "      <td>I said I started in the martial arts world at 17.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7575759.0</td>\n",
       "      <td>Tu as vu juste. Je suis lesbienne.</td>\n",
       "      <td>As you said, I'm a lesbian.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15537867.0</td>\n",
       "      <td>Ainsi que les cinq lits électriques réglables ...</td>\n",
       "      <td>The five electric adjustable beds we bought fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3771038.0</td>\n",
       "      <td>Dix minutes et ils seront de retour et le cham...</td>\n",
       "      <td>Ten minutes and then they'll be back, and the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           sentence  \\\n",
       "0  16271971.0  Peut-être qu'elle paye toutes les vacheries qu...   \n",
       "1   5690632.0  J'ai dit que j'ai commencé les arts martiaux à...   \n",
       "2   7575759.0                 Tu as vu juste. Je suis lesbienne.   \n",
       "3  15537867.0  Ainsi que les cinq lits électriques réglables ...   \n",
       "4   3771038.0  Dix minutes et ils seront de retour et le cham...   \n",
       "\n",
       "                                 translated_sentence  \n",
       "0  Maybe it was her comeuppance for all the bad s...  \n",
       "1  I said I started in the martial arts world at 17.  \n",
       "2                        As you said, I'm a lesbian.  \n",
       "3  The five electric adjustable beds we bought fo...  \n",
       "4  Ten minutes and then they'll be back, and the ...  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_word_counts = pd.DataFrame(columns=['word', 'count'])\n",
    "unique_word_counts_dict = defaultdict(int)\n",
    "\n",
    "for sentence in df['sentence'].values:\n",
    "\n",
    "    #unique_word_counts = scan_sentence(sentence, unique_word_counts)\n",
    "    unique_word_counts_dict = scan_sentence(sentence, unique_word_counts_dict)\n",
    "\n",
    "# Convert the dictionary to a list of tuples\n",
    "unique_word_counts = list(unique_word_counts_dict.items())\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "unique_word_counts = pd.DataFrame(unique_word_counts, columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toby Usher\\AppData\\Local\\Temp\\ipykernel_16488\\4068569241.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[146695.45]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  unique_word_counts.loc[unique_word_counts['word'] == 'le', 'count'] += l_count * le_frequency\n"
     ]
    }
   ],
   "source": [
    "# Need to distribute l appropriately between le and la counts. Based off my previous counts la occurs\n",
    "# 55% of the time\n",
    "if (constants.language_code == 'fr'):\n",
    "\n",
    "    le_frequency = 0.45\n",
    "\n",
    "    l_count = unique_word_counts[unique_word_counts['word'] == 'l']['count'].values[0]\n",
    "\n",
    "    unique_word_counts.loc[unique_word_counts['word'] == 'le', 'count'] += l_count * le_frequency\n",
    "    unique_word_counts.loc[unique_word_counts['word'] == 'la','count'] += l_count * (1 - le_frequency)\n",
    "\n",
    "    # Remove l row\n",
    "    unique_word_counts = unique_word_counts.drop(\n",
    "        unique_word_counts[unique_word_counts['word'] == 'l'].index\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# A parallel approach to creating the unique word counts for when the dataset becomes too big to feasibly handle\\n# with a single python process\\n\\nimport numpy as np\\nimport multiprocessing\\n\\n# check how many cores are available\\nnum_cpus = multiprocessing.cpu_count()\\n\\nnum_chunks = 4\\n\\nif num_cpus < num_chunks:\\n    raise SystemError(f\\'Insufficient number of CPUs ({num_cpus}) for chosen number of chunks ({num_chunks})\\')\\n\\n# split dataset into chunks\\nchunks = np.array_split(df, num_chunks)\\n\\n# Define the function to be run in each process\\ndef process_chunk(chunk):\\n    \"\"\"Calculates the unique word counts for a given chunk of the sentences\\n    \"\"\"\\n\\n    unique_word_counts = pd.DataFrame(columns=[\\'word\\', \\'count\\'])\\n\\n    # Perform your operations on the chunk here\\n    # For example, compute the mean of each column\\n    for sentence in chunk[\\'sentence\\'].values:\\n\\n        unique_word_counts = scan_sentence(sentence, unique_word_counts)\\n\\n    return unique_word_counts\\n\\n# Create a pool of processes\\nwith multiprocessing.Pool(num_chunks) as p:\\n    # Apply the function to each chunk in the pool of processes\\n    results = p.map(process_chunk, chunks)\\n\\n# Now \\'results\\' is a list of the results from each process\\nfor i, chunk_unique_word_counts in enumerate(results):\\n    pass\\n    # TODO: Combine the unique word count dataframes from each process into one and save\\n\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# A parallel approach to creating the unique word counts for when the dataset becomes too big to feasibly handle\n",
    "# with a single python process\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "# check how many cores are available\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "num_chunks = 4\n",
    "\n",
    "if num_cpus < num_chunks:\n",
    "    raise SystemError(f'Insufficient number of CPUs ({num_cpus}) for chosen number of chunks ({num_chunks})')\n",
    "\n",
    "# split dataset into chunks\n",
    "chunks = np.array_split(df, num_chunks)\n",
    "\n",
    "# Define the function to be run in each process\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Calculates the unique word counts for a given chunk of the sentences\n",
    "    \"\"\"\n",
    "\n",
    "    unique_word_counts = pd.DataFrame(columns=['word', 'count'])\n",
    "\n",
    "    # Perform your operations on the chunk here\n",
    "    # For example, compute the mean of each column\n",
    "    for sentence in chunk['sentence'].values:\n",
    "\n",
    "        unique_word_counts = scan_sentence(sentence, unique_word_counts)\n",
    "\n",
    "    return unique_word_counts\n",
    "\n",
    "# Create a pool of processes\n",
    "with multiprocessing.Pool(num_chunks) as p:\n",
    "    # Apply the function to each chunk in the pool of processes\n",
    "    results = p.map(process_chunk, chunks)\n",
    "\n",
    "# Now 'results' is a list of the results from each process\n",
    "for i, chunk_unique_word_counts in enumerate(results):\n",
    "    pass\n",
    "    # TODO: Combine the unique word count dataframes from each process into one and save\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at the dataset, most words that appear three times or fewer in the dataset are either typos or\n",
    "sufficiently obscure that I should probably remove any sentences that contain these words. The logarithmic\n",
    "nature of word frequency distributions in a text corpus means this will cause the number of unique words in\n",
    "the corpus to drop significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of times a word has to appear in the corpus for it to be kept\n",
    "obscurity_cutoff = 16\n",
    "\n",
    "# Get list of words that occur less than this in the dataset\n",
    "obscure_words = unique_word_counts[unique_word_counts['count'] < obscurity_cutoff]['word'].to_list()\n",
    "\n",
    "# Convert the list of words to a set for faster lookup\n",
    "obscure_words = set(obscure_words)\n",
    "\n",
    "# Split the sentences into words (Must use the same regex as was used to create the original word frequency list)\n",
    "df['words'] = df['sentence'].apply(lambda x: re.findall(regex, x.lower()))\n",
    "\n",
    "# Remove any rows where the sentence contains one of the obscure words\n",
    "df = df[~df['words'].apply(lambda x: any(word in obscure_words for word in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     450766\n",
       "sentence               450766\n",
       "translated_sentence    450766\n",
       "words                  450766\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has removed hundreds of thousands of sentences from the dataset and drastically reduced the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>translated_sentence</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5690632.0</td>\n",
       "      <td>J'ai dit que j'ai commencé les arts martiaux à...</td>\n",
       "      <td>I said I started in the martial arts world at 17.</td>\n",
       "      <td>[j, ai, dit, que, j, ai, commencé, les, arts, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7575759.0</td>\n",
       "      <td>Tu as vu juste. Je suis lesbienne.</td>\n",
       "      <td>As you said, I'm a lesbian.</td>\n",
       "      <td>[tu, as, vu, juste, je, suis, lesbienne]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3771038.0</td>\n",
       "      <td>Dix minutes et ils seront de retour et le cham...</td>\n",
       "      <td>Ten minutes and then they'll be back, and the ...</td>\n",
       "      <td>[dix, minutes, et, ils, seront, de, retour, et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40248129.0</td>\n",
       "      <td>Andre a vendu des secrets aux soviétiques.</td>\n",
       "      <td>Andre here sold secrets to the Soviets.</td>\n",
       "      <td>[andre, a, vendu, des, secrets, aux, soviétiques]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17734628.0</td>\n",
       "      <td>Je vous en prie, laissez-moi partir.</td>\n",
       "      <td>Please, please let me go.</td>\n",
       "      <td>[je, vous, en, prie, laissez, moi, partir]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                           sentence  \\\n",
       "1   5690632.0  J'ai dit que j'ai commencé les arts martiaux à...   \n",
       "2   7575759.0                 Tu as vu juste. Je suis lesbienne.   \n",
       "4   3771038.0  Dix minutes et ils seront de retour et le cham...   \n",
       "6  40248129.0         Andre a vendu des secrets aux soviétiques.   \n",
       "7  17734628.0               Je vous en prie, laissez-moi partir.   \n",
       "\n",
       "                                 translated_sentence  \\\n",
       "1  I said I started in the martial arts world at 17.   \n",
       "2                        As you said, I'm a lesbian.   \n",
       "4  Ten minutes and then they'll be back, and the ...   \n",
       "6            Andre here sold secrets to the Soviets.   \n",
       "7                          Please, please let me go.   \n",
       "\n",
       "                                               words  \n",
       "1  [j, ai, dit, que, j, ai, commencé, les, arts, ...  \n",
       "2           [tu, as, vu, juste, je, suis, lesbienne]  \n",
       "4  [dix, minutes, et, ils, seront, de, retour, et...  \n",
       "6  [andre, a, vendu, des, secrets, aux, soviétiques]  \n",
       "7         [je, vous, en, prie, laissez, moi, partir]  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nconn = engine.connect()\\n\\n# create a metadata object and reflect the table\\nmetadata = MetaData()\\ntable = Table('api_appuser_known_words', metadata, autoload_with=engine)\\n\\n# create a delete object and execute it to remove the known words table first\\ndelete_stmt = table.delete()\\nconn.execute(delete_stmt)\\n\\n# Now update the word data table with the new unique words\\nsorted_word_counts.to_sql(f'language_app_{constants.language_code}worddata', engine, if_exists='replace')\\n\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "\n",
    "# Remove the abscure words from the word counts dataframe and save new dataset and word counts\n",
    "unique_word_counts = unique_word_counts[unique_word_counts['count'] >= obscurity_cutoff]\n",
    "\n",
    "# Remove NaN\n",
    "unique_words_counts = unique_word_counts[unique_word_counts['word'].isna()]\n",
    "\n",
    "# Sort and save the frequency list into final tables\n",
    "sorted_word_counts = unique_word_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "#sorted_word_counts['rank'] = sorted_word_counts['count'].rank(ascending=False)\n",
    "\n",
    "# Reset the index\n",
    "sorted_word_counts = sorted_word_counts.reset_index(drop=True)\n",
    "\n",
    "sorted_word_counts['rank'] = sorted_word_counts.index + 1\n",
    "\n",
    "sorted_word_counts = sorted_word_counts[['rank', 'word', 'count']]\n",
    "\n",
    "sorted_word_counts.to_csv(f'../output_files/{constants.language_code}/step4_unique_word_counts.csv', sep='\\t')\n",
    "\n",
    "# Update table in sqlalchemy------------------------------------\n",
    "\n",
    "# Note that you first have to delete the many to many 'words_known' relationship between\n",
    "# users and the word data, so use with caution.\n",
    "engine = create_engine('postgresql://quivo_default:s567tyug328726hj9j83@localhost:5432/quivo')\n",
    "'''\n",
    "conn = engine.connect()\n",
    "\n",
    "# create a metadata object and reflect the table\n",
    "metadata = MetaData()\n",
    "table = Table('api_appuser_known_words', metadata, autoload_with=engine)\n",
    "\n",
    "# create a delete object and execute it to remove the known words table first\n",
    "delete_stmt = table.delete()\n",
    "conn.execute(delete_stmt)\n",
    "\n",
    "# Now update the word data table with the new unique words\n",
    "sorted_word_counts.to_sql(f'language_app_{constants.language_code}worddata', engine, if_exists='replace')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Keep every 100th value for quicker plotting\n",
    "#sorted_word_counts[sorted_word_counts['count'] > 1]['count'].plot(kind='bar')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating word frequency-based sentence complexity scores\n",
    "\n",
    "Now we know all word frequencies, this information can be used to figure out the average sentence complexity of words within each sentence to get a preliminary idea of which sentences will be more difficult to understand / more or less useful for a language learner at any given level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toby Usher\\AppData\\Local\\Temp\\ipykernel_16488\\3641391163.py:18: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  df[['average_count', 'min_count']] = df['words'].apply(calculate_average_and_min, args=(unique_word_counts_dict,))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use the original word counts dict (Still contains the obscure words but should still be quicker)\n",
    "# to calculate the average and min word counts for each sentence\n",
    "\n",
    "def calculate_average_and_min(row, word_counts):\n",
    "\n",
    "    # Gets all counts for the sentence. Defaults to zero if word missing although this should\n",
    "    # never happen\n",
    "    counts = [word_counts.get(word, 0) for word in row]\n",
    "\n",
    "    if not counts:\n",
    "        return pd.Series([0, 0])\n",
    "    \n",
    "    return pd.Series([np.mean(counts), np.min(counts)])\n",
    "\n",
    "# Mean of all word count frequencies, minimum word frequency (rarest word) in sentence\n",
    "df[['average_count', 'min_count']] = df['words'].apply(calculate_average_and_min, args=(unique_word_counts_dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rank columns for average and min counts\n",
    "df['average_count_rank'] = df['average_count'].rank(ascending=True)\n",
    "df['min_count_rank'] = df['min_count'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' not in df.columns:\n",
    "    df['cluster'] = 0\n",
    "\n",
    "# Add in placeholder for translation for now\n",
    "#df['translated_sentence'] = 'Translation'\n",
    "\n",
    "# Remove any translations with a tab in\n",
    "df['translated_sentence'] = df['translated_sentence'].str.replace(r'.*\\t.*', '', regex=True)\n",
    "\n",
    "# Put the dataframe in the correct format for the Django model\n",
    "df = df[[\n",
    "    'sentence',\n",
    "    'translated_sentence',\n",
    "    'cluster',\n",
    "    'words',\n",
    "    'average_count',\n",
    "    'min_count',\n",
    "    'average_count_rank',\n",
    "    'min_count_rank'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "ordered_df = df.sort_values(by='min_count', ascending=True)\n",
    "ordered_df.to_csv(f'../output_files/{constants.language_code}/step4_sentences.csv', sep='\\t')\n",
    "\n",
    "ordered_df.to_sql(f'language_app_{constants.language_code}sentence', engine, if_exists='replace')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>translated_sentence</th>\n",
       "      <th>cluster</th>\n",
       "      <th>words</th>\n",
       "      <th>average_count</th>\n",
       "      <th>min_count</th>\n",
       "      <th>average_count_rank</th>\n",
       "      <th>min_count_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J'ai dit que j'ai commencé les arts martiaux à...</td>\n",
       "      <td>I said I started in the martial arts world at 17.</td>\n",
       "      <td>0</td>\n",
       "      <td>[j, ai, dit, que, j, ai, commencé, les, arts, ...</td>\n",
       "      <td>34863.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137793.5</td>\n",
       "      <td>404233.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571304</th>\n",
       "      <td>C'est ce que j'ai toujours dit.</td>\n",
       "      <td>I've been saying that all along. You're pretty...</td>\n",
       "      <td>0</td>\n",
       "      <td>[c, est, ce, que, j, ai, toujours, dit]</td>\n",
       "      <td>50863.875000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285820.0</td>\n",
       "      <td>404233.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223082</th>\n",
       "      <td>Alors, comme d'habitude, quelqu'un est mort.</td>\n",
       "      <td>And as usual, somebody got dead.</td>\n",
       "      <td>0</td>\n",
       "      <td>[alors, comme, d, habitude, quelqu'un, est, mort]</td>\n",
       "      <td>19649.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32306.0</td>\n",
       "      <td>404233.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223090</th>\n",
       "      <td>C'est là qu'on se fait des choses.</td>\n",
       "      <td>That's where we do things to each other.</td>\n",
       "      <td>0</td>\n",
       "      <td>[c, est, là, qu, on, se, fait, des, choses]</td>\n",
       "      <td>31171.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106417.0</td>\n",
       "      <td>404233.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223091</th>\n",
       "      <td>Qu'est-ce qu'il y a comme pigeons dans cette b...</td>\n",
       "      <td>There's a lot of pigeons in this damn place!</td>\n",
       "      <td>0</td>\n",
       "      <td>[qu, est, ce, qu, il, y, a, comme, pigeons, da...</td>\n",
       "      <td>38431.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>170355.0</td>\n",
       "      <td>404233.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 sentence  \\\n",
       "1       J'ai dit que j'ai commencé les arts martiaux à...   \n",
       "571304                    C'est ce que j'ai toujours dit.   \n",
       "223082       Alors, comme d'habitude, quelqu'un est mort.   \n",
       "223090                 C'est là qu'on se fait des choses.   \n",
       "223091  Qu'est-ce qu'il y a comme pigeons dans cette b...   \n",
       "\n",
       "                                      translated_sentence  cluster  \\\n",
       "1       I said I started in the martial arts world at 17.        0   \n",
       "571304  I've been saying that all along. You're pretty...        0   \n",
       "223082                   And as usual, somebody got dead.        0   \n",
       "223090           That's where we do things to each other.        0   \n",
       "223091       There's a lot of pigeons in this damn place!        0   \n",
       "\n",
       "                                                    words  average_count  \\\n",
       "1       [j, ai, dit, que, j, ai, commencé, les, arts, ...   34863.000000   \n",
       "571304            [c, est, ce, que, j, ai, toujours, dit]   50863.875000   \n",
       "223082  [alors, comme, d, habitude, quelqu'un, est, mort]   19649.142857   \n",
       "223090        [c, est, là, qu, on, se, fait, des, choses]   31171.000000   \n",
       "223091  [qu, est, ce, qu, il, y, a, comme, pigeons, da...   38431.166667   \n",
       "\n",
       "        min_count  average_count_rank  min_count_rank  \n",
       "1             0.0            137793.5        404233.5  \n",
       "571304        0.0            285820.0        404233.5  \n",
       "223082        0.0             32306.0        404233.5  \n",
       "223090        0.0            106417.0        404233.5  \n",
       "223091        0.0            170355.0        404233.5  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the correct constants to describe the frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rank         word     count\n",
      "0          1           de  248091.0\n",
      "10        11           ce   98918.0\n",
      "20        21           on   57095.0\n",
      "30        31         avec   32479.0\n",
      "40        41         tout   24973.0\n",
      "...      ...          ...       ...\n",
      "15880  15881     infernal      16.0\n",
      "15890  15891      décédés      16.0\n",
      "15900  15901      morning      16.0\n",
      "15910  15911  attaqueront      16.0\n",
      "15920  15921    incarcéré      16.0\n",
      "\n",
      "[1593 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get every tenth word up to the nth most frequent word\n",
    "decimated_sorted_word_counts = sorted_word_counts[\n",
    "    sorted_word_counts.index % 10 == 0\n",
    "    ]\n",
    "\n",
    "decimated_sorted_word_counts = decimated_sorted_word_counts[\n",
    "    decimated_sorted_word_counts.index < 30000\n",
    "    ]\n",
    "\n",
    "print(decimated_sorted_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.75483839e+02  2.82197217e+02  9.66836646e-01  1.39194611e-01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "decimated_sorted_word_counts['cum_count'] = decimated_sorted_word_counts['count'].cumsum()\n",
    "\n",
    "# Normalise the cumulative count to between 0-100\n",
    "decimated_sorted_word_counts['norm_cum_count'] = (\n",
    "    decimated_sorted_word_counts['cum_count'] / decimated_sorted_word_counts['cum_count'].max()\n",
    "    ) * 100\n",
    "\n",
    "#centimated_df.count()\n",
    "decimated_sorted_word_counts['norm_cum_count'].plot(kind='line')\n",
    "\n",
    "def f(x, a, b, c, d):\n",
    "    return a + b / (1 + np.exp(-c * x**d))\n",
    "\n",
    "# Need to approximate the function for this distribution\n",
    "x = decimated_sorted_word_counts['rank']\n",
    "real_y = decimated_sorted_word_counts['norm_cum_count']\n",
    "\n",
    "initial_guess = [-100, 200, 0.4, 0.23]\n",
    "\n",
    "# Perform the curve fit\n",
    "(params, params_covariance) = curve_fit(f, x, real_y, p0=initial_guess)\n",
    "\n",
    "# Print the best-fit parameters\n",
    "print(params)\n",
    "\n",
    "approx_y = f(x, *params)\n",
    "\n",
    "# Overlay the function\n",
    "plt.plot(x, approx_y, color='r')  # 'r'\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real distribution can be very closely approximated using the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Now the parameters of the function (a, b, c, d) have to be saved for the language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Get only sentences whose words are all in the top 1000 most frequent words in the corpus\\nthreshold_count = sorted_word_counts[\\n    (sorted_word_counts.index > 999.0) & (sorted_word_counts.index < 1001.0)\\n    ]['count'].values[0]\\n\\ntop_thousand_word_sentences = df[df['min_count'] >= threshold_count]\\n\\ntop_thousand_word_sentences.head(50)\\n\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Get only sentences whose words are all in the top 1000 most frequent words in the corpus\n",
    "threshold_count = sorted_word_counts[\n",
    "    (sorted_word_counts.index > 999.0) & (sorted_word_counts.index < 1001.0)\n",
    "    ]['count'].values[0]\n",
    "\n",
    "top_thousand_word_sentences = df[df['min_count'] >= threshold_count]\n",
    "\n",
    "top_thousand_word_sentences.head(50)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\n\\ndf['norm_min_count'] = df['min_count'] / df['min_count'].max()\\ndf['norm_average_count'] = df['average_count'] / df['average_count'].max()\\n\\n# Plot the distributions of min scores\\ncentimated_df = df[df.index % 100 == 0].sort_values(by='norm_min_count')['norm_min_count']\\n\\n#centimated_df.count()\\ncentimated_df.plot(kind='bar')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From experimenting it seems like sorting by the min word count gives a better estimation of the\n",
    "# complexity of the sentence, but sometimes a simple sentence will contain one very obscure word\n",
    "# that unjustly shoots its value up. Taking the average word frequency often does the opposite:\n",
    "# Prioritises sentences that may contain lots of simple linking words such as de and la even if\n",
    "# the sentence may contain some complex words. A weighting of the two might be more indicative.\n",
    "# Would have to normalise their distributions first.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['norm_min_count'] = df['min_count'] / df['min_count'].max()\n",
    "df['norm_average_count'] = df['average_count'] / df['average_count'].max()\n",
    "\n",
    "# Plot the distributions of min scores\n",
    "centimated_df = df[df.index % 100 == 0].sort_values(by='norm_min_count')['norm_min_count']\n",
    "\n",
    "#centimated_df.count()\n",
    "centimated_df.plot(kind='bar')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot the distributions of min scores\\ncentimated_df = df[df.index % 100 == 0].sort_values(by='norm_average_count')['norm_average_count']\\n\\n#centimated_df.count()\\ncentimated_df.plot(kind='bar')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Plot the distributions of min scores\n",
    "centimated_df = df[df.index % 100 == 0].sort_values(by='norm_average_count')['norm_average_count']\n",
    "\n",
    "#centimated_df.count()\n",
    "centimated_df.plot(kind='bar')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I remove the extreme valuesthe normalised distribution of average frequency scores is pretty linear, whereas the distribution of minimum values (The rarest word in the sentence) is logarithmic. I therefore need to apply a transformation to one to enable a weighted average to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Remove sentences whose average counts are within the top or bottom 10% and normalise so that\\n# highest value equal to 1 and lowest value equal to zero\\nbottom_threshold = df.quantile(0.1)\\ntop_threshold = df.quantile(0.9)\\n\\ndf = df[\\n    df['norm_average_count'] >= bottom_threshold\\n    & df['norm_average_count'] <= top_threshold\\n    ]\\n\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Remove sentences whose average counts are within the top or bottom 10% and normalise so that\n",
    "# highest value equal to 1 and lowest value equal to zero\n",
    "bottom_threshold = df.quantile(0.1)\n",
    "top_threshold = df.quantile(0.9)\n",
    "\n",
    "df = df[\n",
    "    df['norm_average_count'] >= bottom_threshold\n",
    "    & df['norm_average_count'] <= top_threshold\n",
    "    ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
