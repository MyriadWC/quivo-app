{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to go through my cleaned word list, split all sentences into their individual words, and create a new dataframe containing each individual word and the number of occurrences within the dataset to determine dataset word frequency (Which I will take as a proxy for overall word frequency). This isn't perfect and will ignore context-defined meaning in words that are spelled the same, as this will be counted as only one highly frequent word with multiple meanings rather than several less-frequent words that are spelled the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few issues will need to be solved here.\n",
    "\n",
    "1. There will be typos and misspellings in the dataset which as well as being less useful as a learning tool, will also lead to some words (particularly those with accents that have been forgotten or misplaced) being counted multiple times, so they'll somehow need to be joined together. I can't just remove all accents as this can change the meaning of a word entirely (côté vs. côte). I might have to reduce the number of sentences in the active dataset and spell check them more thoroughly with grammarly or some other tool.\n",
    "\n",
    "2. Words that have been shortened due to consecutive vowels (l, d etc. could have multiple meanings such as le or la, or de or du), but since these are incredibly common words that will always have high scores it might not be so much of an issue for gauging the word's frequency. I just need to make sure to check less frequent words that have been shortened i.e. the presque in presqu'île, although I imagine this shortening would be sufficiently infrequent relative to its unshortened form as to have almost no bearing on its frequency score.\n",
    "\n",
    "3. I might have to set some custom rules for when to keep an apostrophe. Returning to my previous example, presqu'île should really be treated as a single word, and other such exceptions will exist. I just need to go down the frequency list and note them down.\n",
    "\n",
    "4. Different word types will have differing numbers of forms. Adjectives can be masculine or feminine (public, publique), plus others such as publiquement. Verbs can have a very large number of forms, meaning etre is likely to be separated far more than most adjectives which will have fewer forms, which could lead to forms of etre appearing below far less common words in the frequency list. For a language learner these forms can sometimes be similar in form, allowing a transfer of knowledge when learning new forms (If they know courir it doesn't take much to figure out that couru is just the past tense), while other times the different forms might represent entirely new strings of characters that must all be learnt independently from one another (aller, irai etc.). I'd say that since these are effectively different words that each have to be learned, it is reasonable that they are treated separately. I'm effectively mapping the frequency of different strings of letters appearing in a language.\n",
    "\n",
    "Solution:\n",
    "\n",
    "Overall, while this method for measuring word frequency is flawed in multiple ways, as long as I ensure that the sentences in my database are grammatically correct it should be useful enough as a tool to gauge sentence complexity via average word frequency, which is the ultimate aim. I could also use POS tagging by running each sentence through a pretrained hidden markov model or something similar to label word type, and have the frequency for each word+type pair. This would avoid the issue of words with multiple meanings being classified together (est for is and east etc.). I have to do this later for the single-word translation to work so it would make sense to do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some weird caching issue so overriding for now\n",
    "constants.language_code = 'de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"../output_files/{constants.language_code}/step3_sentences.csv\"\n",
    "\n",
    "df = pd.read_csv(filepath, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          1843728\n",
       "sentence    1843728\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regex\n",
    "\n",
    "fr_exceptions = [\n",
    "    \"[Aa]ujourd'hui\",\n",
    "    \"[Pp]resqu'île\",\n",
    "    \"[Qq]uelqu'un\",\n",
    "    \"[Dd]'accord\"\n",
    "    ]\n",
    "\n",
    "de_exceptions = []\n",
    "\n",
    "exceptions = {\n",
    "    'fr': fr_exceptions,\n",
    "    'de': de_exceptions\n",
    "    }[constants.language_code]\n",
    "\n",
    "word_regex = {\n",
    "    'fr': r'[a-zA-ZéèêëÉÈÊËàâäÀÂÄôöÔÖûüÛÜçÇîÎïÏ]+',\n",
    "    'de': r'[a-zA-ZäöüÄÖÜß]+'\n",
    "    }[constants.language_code]\n",
    "\n",
    "exceptions_regex = '|'.join(exceptions)\n",
    "\n",
    "# Not ideal but de empty exceptions was causing issues\n",
    "regex = {\n",
    "    'fr': fr'\\b{exceptions_regex}|{word_regex}\\b',\n",
    "    'de': fr'\\b{word_regex}\\b',\n",
    "    }[constants.language_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortened_word_map = {\n",
    "    'j': 'je',\n",
    "    #'l': 'le', # Can be either so will handle after to speed up function\n",
    "    't': 'tu', # This will assign the t in a-t-on to tu for example, which will give tu a higher frequency than it should have, but it's only one very common word so I'm not going to address it\n",
    "    'd': 'de', # Need to check whether this is ever du\n",
    "    'c': 'ce',\n",
    "    's': 'se',\n",
    "    'qu': 'que',\n",
    "    'm': 'me',\n",
    "    'n': 'ne',\n",
    "    }\n",
    "\n",
    "def scan_sentence(sentence: str, unique_word_counts: defaultdict) -> defaultdict:\n",
    "    '''Scans a sentence to get its words and updates the unique word count dictionary.\n",
    "    Local unique_word_counts points to global variable so can update directly. The use\n",
    "    of a default dict means we don't have to check if a key is in the dictionary before\n",
    "    adding it as it will initialise to 1.\n",
    "\n",
    "    Using a dict instead of a dataframe means there's O(1) time complexity for insertions\n",
    "    and lookups, and using a defaultdict to avoid an additional check means this runs\n",
    "    incredibly quickly on even very large datasets.\n",
    "    '''\n",
    "\n",
    "    # Split all words in the sentence by word boundaries (Split uninclusively at punctuation or non-alphanumeric characters)\n",
    "    words = re.findall(regex, sentence) # Words only\n",
    "\n",
    "    # Set all words to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "\n",
    "    # Replace any shortened words with their full-length version\n",
    "    if constants.language_code == 'fr':\n",
    "        words = [shortened_word_map.get(word, word) for word in words]\n",
    "\n",
    "    for word in words:\n",
    "\n",
    "        # Add 1 to count. If word doesn't exist adds in new entry\n",
    "        unique_word_counts[word] += 1\n",
    "\n",
    "    return unique_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>000 In der Bergwelt schwimmen Der fantastische...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>00.49 Uhr - Nato-Generalsekretär Jens Stoltenb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>007 und die Lizenz zum Geldverdienen Nach zwei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>007 wird zum Steiermark-Botschafter, frohlockt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>01.00 Uhr - Am Dienstag geht das Tor zu an der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           sentence\n",
       "0  13  000 In der Bergwelt schwimmen Der fantastische...\n",
       "1  32  00.49 Uhr - Nato-Generalsekretär Jens Stoltenb...\n",
       "2  39  007 und die Lizenz zum Geldverdienen Nach zwei...\n",
       "3  40  007 wird zum Steiermark-Botschafter, frohlockt...\n",
       "4  52  01.00 Uhr - Am Dienstag geht das Tor zu an der..."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_word_counts = pd.DataFrame(columns=['word', 'count'])\n",
    "unique_word_counts_dict = defaultdict(int)\n",
    "\n",
    "for sentence in df['sentence'].values:\n",
    "\n",
    "    #unique_word_counts = scan_sentence(sentence, unique_word_counts)\n",
    "    unique_word_counts_dict = scan_sentence(sentence, unique_word_counts_dict)\n",
    "\n",
    "# Convert the dictionary to a list of tuples\n",
    "unique_word_counts = list(unique_word_counts_dict.items())\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "unique_word_counts = pd.DataFrame(unique_word_counts, columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to distribute l appropriately between le and la counts. Based off my previous counts la occurs\n",
    "# 55% of the time\n",
    "if (constants.language_code == 'fr'):\n",
    "\n",
    "    le_frequency = 0.45\n",
    "\n",
    "    l_count = unique_word_counts[unique_word_counts['word'] == 'l']['count'].values[0]\n",
    "\n",
    "    unique_word_counts.loc[unique_word_counts['word'] == 'le', 'count'] += l_count * le_frequency\n",
    "    unique_word_counts.loc[unique_word_counts['word'] == 'la','count'] += l_count * (1 - le_frequency)\n",
    "\n",
    "    # Remove l row\n",
    "    unique_word_counts = unique_word_counts.drop(\n",
    "        unique_word_counts[unique_word_counts['word'] == 'l'].index\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# A parallel approach to creating the unique word counts for when the dataset becomes too big to feasibly handle\\n# with a single python process\\n\\nimport numpy as np\\nimport multiprocessing\\n\\n# check how many cores are available\\nnum_cpus = multiprocessing.cpu_count()\\n\\nnum_chunks = 4\\n\\nif num_cpus < num_chunks:\\n    raise SystemError(f\\'Insufficient number of CPUs ({num_cpus}) for chosen number of chunks ({num_chunks})\\')\\n\\n# split dataset into chunks\\nchunks = np.array_split(df, num_chunks)\\n\\n# Define the function to be run in each process\\ndef process_chunk(chunk):\\n    \"\"\"Calculates the unique word counts for a given chunk of the sentences\\n    \"\"\"\\n\\n    unique_word_counts = pd.DataFrame(columns=[\\'word\\', \\'count\\'])\\n\\n    # Perform your operations on the chunk here\\n    # For example, compute the mean of each column\\n    for sentence in chunk[\\'sentence\\'].values:\\n\\n        unique_word_counts = scan_sentence(sentence, unique_word_counts)\\n\\n    return unique_word_counts\\n\\n# Create a pool of processes\\nwith multiprocessing.Pool(num_chunks) as p:\\n    # Apply the function to each chunk in the pool of processes\\n    results = p.map(process_chunk, chunks)\\n\\n# Now \\'results\\' is a list of the results from each process\\nfor i, chunk_unique_word_counts in enumerate(results):\\n    pass\\n    # TODO: Combine the unique word count dataframes from each process into one and save\\n\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# A parallel approach to creating the unique word counts for when the dataset becomes too big to feasibly handle\n",
    "# with a single python process\n",
    "\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "# check how many cores are available\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "num_chunks = 4\n",
    "\n",
    "if num_cpus < num_chunks:\n",
    "    raise SystemError(f'Insufficient number of CPUs ({num_cpus}) for chosen number of chunks ({num_chunks})')\n",
    "\n",
    "# split dataset into chunks\n",
    "chunks = np.array_split(df, num_chunks)\n",
    "\n",
    "# Define the function to be run in each process\n",
    "def process_chunk(chunk):\n",
    "    \"\"\"Calculates the unique word counts for a given chunk of the sentences\n",
    "    \"\"\"\n",
    "\n",
    "    unique_word_counts = pd.DataFrame(columns=['word', 'count'])\n",
    "\n",
    "    # Perform your operations on the chunk here\n",
    "    # For example, compute the mean of each column\n",
    "    for sentence in chunk['sentence'].values:\n",
    "\n",
    "        unique_word_counts = scan_sentence(sentence, unique_word_counts)\n",
    "\n",
    "    return unique_word_counts\n",
    "\n",
    "# Create a pool of processes\n",
    "with multiprocessing.Pool(num_chunks) as p:\n",
    "    # Apply the function to each chunk in the pool of processes\n",
    "    results = p.map(process_chunk, chunks)\n",
    "\n",
    "# Now 'results' is a list of the results from each process\n",
    "for i, chunk_unique_word_counts in enumerate(results):\n",
    "    pass\n",
    "    # TODO: Combine the unique word count dataframes from each process into one and save\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at the dataset, most words that appear three times or fewer in the dataset are either typos or\n",
    "sufficiently obscure that I should probably remove any sentences that contain these words. The logarithmic\n",
    "nature of word frequency distributions in a text corpus means this will cause the number of unique words in\n",
    "the corpus to drop significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of times a word has to appear in the corpus for it to be kept\n",
    "obscurity_cutoff = 16\n",
    "\n",
    "# Get list of words that occur less than this in the dataset\n",
    "obscure_words = unique_word_counts[unique_word_counts['count'] < obscurity_cutoff]['word'].to_list()\n",
    "\n",
    "# Convert the list of words to a set for faster lookup\n",
    "obscure_words = set(obscure_words)\n",
    "\n",
    "# Split the sentences into words (Must use the same regex as was used to create the original word frequency list)\n",
    "df['words'] = df['sentence'].apply(lambda x: re.findall(regex, x.lower()))\n",
    "\n",
    "# Remove any rows where the sentence contains one of the obscure words\n",
    "df = df[~df['words'].apply(lambda x: any(word in obscure_words for word in x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          961044\n",
       "sentence    961044\n",
       "words       961044\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has removed hundreds of thousands of sentences from the dataset and drastically reduced the number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>01.00 Uhr - Am Dienstag geht das Tor zu an der...</td>\n",
       "      <td>[uhr, am, dienstag, geht, das, tor, zu, an, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>01031 Suchen Europäer bald in Russland Schutz ...</td>\n",
       "      <td>[suchen, europäer, bald, in, russland, schutz,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61</td>\n",
       "      <td>01.03 Uhr - Präsident Francois Holland sagt se...</td>\n",
       "      <td>[uhr, präsident, francois, holland, sagt, sein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74</td>\n",
       "      <td>01.06 Uhr - Präsident Francois Holland sagt se...</td>\n",
       "      <td>[uhr, präsident, francois, holland, sagt, sein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>109</td>\n",
       "      <td>01.25 Uhr - Deutsche Topmanager dringen einer ...</td>\n",
       "      <td>[uhr, deutsche, topmanager, dringen, einer, um...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                           sentence  \\\n",
       "4   52  01.00 Uhr - Am Dienstag geht das Tor zu an der...   \n",
       "5   60  01031 Suchen Europäer bald in Russland Schutz ...   \n",
       "6   61  01.03 Uhr - Präsident Francois Holland sagt se...   \n",
       "7   74  01.06 Uhr - Präsident Francois Holland sagt se...   \n",
       "9  109  01.25 Uhr - Deutsche Topmanager dringen einer ...   \n",
       "\n",
       "                                               words  \n",
       "4  [uhr, am, dienstag, geht, das, tor, zu, an, de...  \n",
       "5  [suchen, europäer, bald, in, russland, schutz,...  \n",
       "6  [uhr, präsident, francois, holland, sagt, sein...  \n",
       "7  [uhr, präsident, francois, holland, sagt, sein...  \n",
       "9  [uhr, deutsche, topmanager, dringen, einer, um...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nconn = engine.connect()\\n\\n# create a metadata object and reflect the table\\nmetadata = MetaData()\\ntable = Table('api_appuser_known_words', metadata, autoload_with=engine)\\n\\n# create a delete object and execute it to remove the known words table first\\ndelete_stmt = table.delete()\\nconn.execute(delete_stmt)\\n\\n# Now update the word data table with the new unique words\\nsorted_word_counts.to_sql(f'language_app_{constants.language_code}worddata', engine, if_exists='replace')\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Table, MetaData\n",
    "\n",
    "# Remove the abscure words from the word counts dataframe and save new dataset and word counts\n",
    "unique_word_counts = unique_word_counts[unique_word_counts['count'] >= obscurity_cutoff]\n",
    "\n",
    "# Sort and save the frequency list into final tables\n",
    "sorted_word_counts = unique_word_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Reset index to be the frequency rank\n",
    "sorted_word_counts['rank'] = sorted_word_counts['count'].rank(ascending=False)\n",
    "\n",
    "sorted_word_counts = sorted_word_counts[['rank', 'word', 'count']]\n",
    "\n",
    "sorted_word_counts.to_csv(f'../output_files/{constants.language_code}/step4_unique_word_counts.csv', sep='\\t')\n",
    "\n",
    "# Update table in sqlalchemy------------------------------------\n",
    "\n",
    "# Note that you first have to delete the many to many 'words_known' relationship between\n",
    "# users and the word data, so use with caution.\n",
    "engine = create_engine('postgresql://quivo_default:s567tyug328726hj9j83@localhost:5432/quivo')\n",
    "'''\n",
    "conn = engine.connect()\n",
    "\n",
    "# create a metadata object and reflect the table\n",
    "metadata = MetaData()\n",
    "table = Table('api_appuser_known_words', metadata, autoload_with=engine)\n",
    "\n",
    "# create a delete object and execute it to remove the known words table first\n",
    "delete_stmt = table.delete()\n",
    "conn.execute(delete_stmt)\n",
    "\n",
    "# Now update the word data table with the new unique words\n",
    "sorted_word_counts.to_sql(f'language_app_{constants.language_code}worddata', engine, if_exists='replace')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Keep every 100th value for quicker plotting\n",
    "#sorted_word_counts[sorted_word_counts['count'] > 1]['count'].plot(kind='bar')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating word frequency-based sentence complexity scores\n",
    "\n",
    "Now we know all word frequencies, this information can be used to figure out the average sentence complexity of words within each sentence to get a preliminary idea of which sentences will be more difficult to understand / more or less useful for a language learner at any given level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Toby Usher\\AppData\\Local\\Temp\\ipykernel_9180\\3641391163.py:18: FutureWarning: Returning a DataFrame from Series.apply when the supplied function returns a Series is deprecated and will be removed in a future version.\n",
      "  df[['average_count', 'min_count']] = df['words'].apply(calculate_average_and_min, args=(unique_word_counts_dict,))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use the original word counts dict (Still contains the obscure words but should still be quicker)\n",
    "# to calculate the average and min word counts for each sentence\n",
    "\n",
    "def calculate_average_and_min(row, word_counts):\n",
    "\n",
    "    # Gets all counts for the sentence. Defaults to zero if word missing although this should\n",
    "    # never happen\n",
    "    counts = [word_counts.get(word, 0) for word in row]\n",
    "\n",
    "    if not counts:\n",
    "        return pd.Series([0, 0])\n",
    "    \n",
    "    return pd.Series([np.mean(counts), np.min(counts)])\n",
    "\n",
    "# Mean of all word count frequencies, minimum word frequency (rarest word) in sentence\n",
    "df[['average_count', 'min_count']] = df['words'].apply(calculate_average_and_min, args=(unique_word_counts_dict,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rank columns for average and min counts\n",
    "df['average_count_rank'] = df['average_count'].rank(ascending=True)\n",
    "df['min_count_rank'] = df['min_count'].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cluster' not in df.columns:\n",
    "    df['cluster'] = 0\n",
    "\n",
    "# Add in placeholder for translation for now\n",
    "df['translated_sentence'] = 'Translation'\n",
    "\n",
    "# Put the dataframe in the correct format for the Django model\n",
    "df = df[[\n",
    "    'sentence',\n",
    "    'translated_sentence',\n",
    "    'cluster',\n",
    "    'words',\n",
    "    'average_count',\n",
    "    'min_count',\n",
    "    'average_count_rank',\n",
    "    'min_count_rank'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "ordered_df = df.sort_values(by='min_count', ascending=True)\n",
    "ordered_df.to_csv(f'../output_files/{constants.language_code}/step4_sentences.csv', sep='\\t')\n",
    "\n",
    "ordered_df.to_sql(f'language_app_{constants.language_code}sentence', engine, if_exists='replace')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>translated_sentence</th>\n",
       "      <th>cluster</th>\n",
       "      <th>words</th>\n",
       "      <th>average_count</th>\n",
       "      <th>min_count</th>\n",
       "      <th>average_count_rank</th>\n",
       "      <th>min_count_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentence, translated_sentence, cluster, words, average_count, min_count, average_count_rank, min_count_rank]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_df[ordered_df.index == 256541]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Get only sentences whose words are all in the top 1000 most frequent words in the corpus\\nthreshold_count = sorted_word_counts[\\n    (sorted_word_counts.index > 999.0) & (sorted_word_counts.index < 1001.0)\\n    ]['count'].values[0]\\n\\ntop_thousand_word_sentences = df[df['min_count'] >= threshold_count]\\n\\ntop_thousand_word_sentences.head(50)\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Get only sentences whose words are all in the top 1000 most frequent words in the corpus\n",
    "threshold_count = sorted_word_counts[\n",
    "    (sorted_word_counts.index > 999.0) & (sorted_word_counts.index < 1001.0)\n",
    "    ]['count'].values[0]\n",
    "\n",
    "top_thousand_word_sentences = df[df['min_count'] >= threshold_count]\n",
    "\n",
    "top_thousand_word_sentences.head(50)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport matplotlib.pyplot as plt\\n\\ndf['norm_min_count'] = df['min_count'] / df['min_count'].max()\\ndf['norm_average_count'] = df['average_count'] / df['average_count'].max()\\n\\n# Plot the distributions of min scores\\ncentimated_df = df[df.index % 100 == 0].sort_values(by='norm_min_count')['norm_min_count']\\n\\n#centimated_df.count()\\ncentimated_df.plot(kind='bar')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From experimenting it seems like sorting by the min word count gives a better estimation of the\n",
    "# complexity of the sentence, but sometimes a simple sentence will contain one very obscure word\n",
    "# that unjustly shoots its value up. Taking the average word frequency often does the opposite:\n",
    "# Prioritises sentences that may contain lots of simple linking words such as de and la even if\n",
    "# the sentence may contain some complex words. A weighting of the two might be more indicative.\n",
    "# Would have to normalise their distributions first.\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['norm_min_count'] = df['min_count'] / df['min_count'].max()\n",
    "df['norm_average_count'] = df['average_count'] / df['average_count'].max()\n",
    "\n",
    "# Plot the distributions of min scores\n",
    "centimated_df = df[df.index % 100 == 0].sort_values(by='norm_min_count')['norm_min_count']\n",
    "\n",
    "#centimated_df.count()\n",
    "centimated_df.plot(kind='bar')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Plot the distributions of min scores\\ncentimated_df = df[df.index % 100 == 0].sort_values(by='norm_average_count')['norm_average_count']\\n\\n#centimated_df.count()\\ncentimated_df.plot(kind='bar')\\nplt.show()\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Plot the distributions of min scores\n",
    "centimated_df = df[df.index % 100 == 0].sort_values(by='norm_average_count')['norm_average_count']\n",
    "\n",
    "#centimated_df.count()\n",
    "centimated_df.plot(kind='bar')\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I remove the extreme valuesthe normalised distribution of average frequency scores is pretty linear, whereas the distribution of minimum values (The rarest word in the sentence) is logarithmic. I therefore need to apply a transformation to one to enable a weighted average to be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Remove sentences whose average counts are within the top or bottom 10% and normalise so that\\n# highest value equal to 1 and lowest value equal to zero\\nbottom_threshold = df.quantile(0.1)\\ntop_threshold = df.quantile(0.9)\\n\\ndf = df[\\n    df['norm_average_count'] >= bottom_threshold\\n    & df['norm_average_count'] <= top_threshold\\n    ]\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Remove sentences whose average counts are within the top or bottom 10% and normalise so that\n",
    "# highest value equal to 1 and lowest value equal to zero\n",
    "bottom_threshold = df.quantile(0.1)\n",
    "top_threshold = df.quantile(0.9)\n",
    "\n",
    "df = df[\n",
    "    df['norm_average_count'] >= bottom_threshold\n",
    "    & df['norm_average_count'] <= top_threshold\n",
    "    ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
